# Baseline: Sber Annual Report 2022 (2026-02-09)

- Dataset: `eval/questions.jsonl`
- Questions: `24`
- Model: `GigaChat-2-Max`
- Run type: current implementation baseline

## Aggregate

- **weighted_score: 0.6540384615384615**

## Scoring formula

Per-question:

- `include_rate = include_hits / include_total` (если `must_include` пуст, то `1.0`)
- `safe_ok = 1.0`, если нет попаданий по `must_not_include`, иначе `0.0`
- **`question_score = 0.7 * include_rate + 0.3 * safe_ok`**

Overall:

- **`weighted_score = sum(question_score_i * weight_i) / sum(weight_i)`**

## Command

```bash
GIGACHAT_API_KEY=*** ALLOW_DANGEROUS_FAISS_DESERIALIZATION=1 PYTHONPATH=. \
python eval/run_eval.py \
  --pdf eval/data/source.pdf \
  --questions eval/questions.jsonl \
  --out eval/runs/baseline.json
```

## Lowest-scoring questions (for first improvement iteration)

- q002 — score 0.300 (include 0/1)
- q003 — score 0.300 (include 0/1)
- q004 — score 0.300 (include 0/2)
- q005 — score 0.300 (include 0/1)
- q006 — score 0.300 (include 0/2)

> Full raw run is stored locally at `eval/runs/baseline.json` (ignored by git).
