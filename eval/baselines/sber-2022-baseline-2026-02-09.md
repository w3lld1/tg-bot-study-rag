# Baseline: Sber Annual Report 2022 (2026-02-09)

- Dataset: `eval/questions.jsonl`
- Questions: `24`
- Model: `GigaChat-2-Max`
- Run type: current implementation baseline
- `pdf_sha256`: `8a859081ed75174ba1c86a4578b2450c40e517d90a15a4e1f636f94058d3bdb9`
- `questions_sha256`: `0c4f7257604c1ef213efb950c663c4ae8af5f246de0635f3b9b11044232dd9c9`

## Aggregate

- **weighted_score: 0.5948076923076925**

## Lowest-scoring questions (v2 recalculated)

- q003 — score 0.100 (include 0/1, citation_ok=false)
- q004 — score 0.100 (include 0/2, citation_ok=false)
- q005 — score 0.100 (include 0/1, citation_ok=false)
- q006 — score 0.100 (include 0/2, citation_ok=false)
- q007 — score 0.100 (include 0/2, citation_ok=false)

## Scoring formula (v2)

Per-question:

- `include_rate = include_hits / include_total`
- `safe_ok = 1.0`, если нет попаданий по `must_not_include`, иначе `0.0`
- `base_score = 0.7 * include_rate + 0.3 * safe_ok`
- при `require_citation=true` и отсутствии `стр. N` (в скобках или без) применяется `citation_penalty = 0.2`
- `question_score = max(0.0, base_score - citation_penalty)`

Overall:

- `weighted_score = sum(question_score_i * weight_i) / sum(weight_i)`

## Command

```bash
GIGACHAT_API_KEY=*** ALLOW_DANGEROUS_FAISS_DESERIALIZATION=1 PYTHONPATH=. \
python eval/run_eval.py \
  --pdf eval/data/source.pdf \
  --questions eval/questions.jsonl \
  --out eval/runs/baseline.json
```

> Full raw run is stored locally at `eval/runs/baseline.json` (ignored by git).
